---
title: "Homework 5 - Heart Disease Modeling"
author: "Jason M. Pattison, ST 558-601, Summer 1 2024"
format: html
editor: visual
---

# "Homework 5 - Heart Disease Modeling"

## Task 1: Conceptual Questions

1.  What is the purpose of using cross-validation when fitting a random forest model?

> Answer

2.  Describe the bagged tree algorithm.

> Answer

3.  What is meant by a general linear model?

> Answer

4.  When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to do differently as compared to when it is not included in the model?

> Answer

5.  Why do we split our data into a training and test set?

> Answer

## Task 2: Fitting Models

The first thing we'll need to do is establish the libraries required to run the code for each of the models we're going to develop.

```{r download libraries}

library(tidyverse)
library(haven)
library(knitr)
library(caret)
library(tree)
library(randomForest)
library(rgl) 

```

### 2.1 Quick EDA/Data Preparation

2.1.1 Read in heart.csv. Check on missingness and summarize the data, especially with respect to the variable relationships to HeartDisease.

```{r create data frame}

heart_df <- read_csv("https://www4.stat.ncsu.edu/~online/datasets/heart.csv", 
                     show_col_types = FALSE)

heart_df

```

Check on for NA values in the data frame results in

```{r check for missingness}

sum_na <- function(col) {
  sum(is.na(col))
}

heart_df_na_summary <- heart_df |>
  summarize(across(everything(), sum_na))

print(heart_df_na_summary)

```

We see that there are no missing values in the data set.

Next we will summarize the data to determine if there are any irregularities that need to be addressed before further analysis

```{r data structure}

print(summary(heart_df))

```

Review of the numeric variable summaries shows that there is an irregularity in the RestingBP data, and there may or may not be one in the Cholesterol data as well. For the remaining five character variables, we generated contingency tables that will be used to determine if the variable data requires further investigation for data cleaning or not. 

The irregularity in RestingBP is the summary information shows a minimum entry of 0.0. This value is highly unlikely unless the PT observed is already a corpse (or mannequin). First we will use the `sum()` function to determine how many values input for RestingBP are "0".

```{r}
list("RestingBP_eq_zero" = sum(heart_df$RestingBP == 0))

```
We see that there is only one entry that meets this criteria. Because this one entry will have minimal affect on the prediciton models, we may want to consider leaving the PT's information for contribution to the other variables used in generating our training model. 

The Cholesterol summary shows a minimum entry of 0.0, which is also unlikely given every person has some level of total cholesterol when tested for LDL, HDL, and Triglycerides. Like with RestingBP, we will use the `sum()` function to determine how many values for Cholesterol are "0". 

```{r}

list("Cholesterol_eq_zero" = sum(heart_df$Cholesterol == 0))

```

The `sum()` function determined that there are 172 entries with "0" reported for the PTs Cholesterol level. This accounts for approximately 19% of the overall data set. This is a large amount of the sample population, which suggests the use of "0" was intentional instead of "NA" or some other coding. Imputing the mean of the remaining 81% for these values without knowing their distribution across the five data sources risks heavily biasing our prediction modeling. Further summary analysis is requried for assessing what to do with these observations. First, we will use a histogram to determine what the distribution looks like. 

```{r}

ggplot(heart_df, aes(Cholesterol)) +
  geom_histogram(binwidth=10)

```

The histogram confirms the 172 entries of "0" as outliers and identified a second group of outliers to the right of the data in the range of 450 to 600. The rest of the data appears to have a generally normal distribution. 

In order to confirm the Cholesterol outliers, and how they're affecting the HeartDisease variable, we will use box and whisker plots. One for the overall HeartDisease variable, and a second for HeartDisease's sub-groups. The second plot is necessary to assess how the outliers are affecting HeartDisease's "0" group separate from the "1" group. 

```{r}

ggplot(heart_df, aes(HeartDisease, Cholesterol)) +
  geom_boxplot() +
  labs(x = "H")

```

Review of the combined box and whisker plot of HeartDisease confirms the outliers at "0" and in the range of "400" through "600". 

```{r}

ggplot(heart_df, aes(HeartDisease, Cholesterol)) +
  geom_boxplot(aes(group = HeartDisease)) 

```
Review of the sub-group histogram shows that the Cholesterol values of "0" greatly affect the IQR of the "1" HeartDisease subgroup while having minimal to no effect on the "0" HeartDisease subgroup. The Cholesterol values of "0" do not appear to have an affect on the median values of either subgroup as they both are near the overall group median of 223. 

With all visual summary observations taken into consideration, imputing the mean of the 81% of the rest of the Cholesterol values is reasonable after removing the outlying values greater than 400. 

Next we generated a summary table of Sex.

```{r}

print(table("Sex" = heart_df$Sex))

```

The [Kaggle website](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction) reported that this variable only provided a binary option, so there does not appear to be any need to further analyze it.

Next, we generalized a summary table of Chest Pain Type. 

```{r}

print(table("Chest Pain Type" = heart_df$ChestPainType))

```
Each of these factors are consistent with what was reported on the Kaggle site. There does not appear to be any need to further analyze it. 

Next we generated a summary table for Resting ECG. 

```{r}

print(table("Resting ECG" = heart_df$RestingECG))

```

Each of these factors are consistent with what was reported on the Kaggle site. There does not appear to be any need to further analyze it. 

The final variable we generated a summary table for is ExerciseAngina. 

```{r}

print(table("Exercise Angina" = heart_df$ExerciseAngina))

```

The Kaggle website reported that this variable only provided a binary option, so there does not appear to be any need to further analyze it. 

We did not analyze ST_Slope because the variable is being dropped from the data set per the Homework5 instructions. 

Summarize the data set with respect to the variable relationships to HeartDisease.

```{r}

age_plot <- ggplot(heart_df, aes(x = HeartDisease, y = Age)) +
  geom_boxplot(fill="#CC0000", aes(group = HeartDisease))

print(age_plot)

```

```{r}



ggplot(heart_df, aes(x = HeartDisease, y = Sex, fill=Sex)) +
  geom_bar(stat = "identity")
```

```{r}

table(heart_df$HeartDisease, heart_df$ChestPainType)

```











2.1.2 Create a new variable that is a factor of HeartDisease, (1 = Yes, 0 = No) Remove the ST_Slope and HeartDisease variables.

```{r}



```

2.1.3 Set-up the data frame to ensure the variables are all numeric predictors for kNN modeling later in the program.

```{r}



```

### 2.2 Split the data

```{r}



```

### 2.3 kNN

2.3.1 Scale back the data frame to only have numeric variables for kNN modeling.

```{r}



```

2.3.2. Train the model using 10 fold cross-validation with the number of steps being 3.

```{r}



```

Set the `tuneGrid` so that the values considered are k = 1:40. Check how well the chosen model does on the test set using the `confusionMatrix` function.

### 2.4 Logistic Regression

2.4.1 Posit three different logistic regression models using the EDA from section 2.1. Because the `glm` function can handle factor and character type variables. do not need to include the dummy variables created in section 2.1.2

```{r}



```

Fit the models on the training set. Use CV with the same parameters used in the kNN model. **Note to self: Pre-process the data or no???**

```{r}



```

**Identify the best model and provide a basic `summary` of it.**

Check how well the chosen model does on the test set using the `confusionMatrix` function.

```{r}



```

### 2.5 Tree Models

2.5.1 Choose the variables of interest and use repeated 10 fold CV to select a best fit.

-   classificaiton tree (use method = rpart: tuning parameter is cp, use values 1, 0.001, 0.002, ..., 0.1)

```{r}



```

-   random forest (use method = rf: tuning parameter is mtry, use values of 1, 2, ..., \# of predictors) **bagging is a special case in this model** -

```{r}



```

-   boosted tree (use method = gbm: tuning parameters are n.trees, interaction.depth, shrinkage, and n.minabsinnode). Use all combinations of n.trees of 25, 50, 100, and 200; interaction.depth of 1, 2, 3; shrinkage = 0.1; and nminobsinnode = 10.**Use `expand.grid` to create the data frame for `tuneGrid` and verbose = FALSE limits the output produced**.

```{r}



```

Check how each of the above chosen models do on the test set using the `confusionMatrix` function.

```{r}



```

### Wrap up

Which model overall did the best job (in terms of accuracy) on the test set?
